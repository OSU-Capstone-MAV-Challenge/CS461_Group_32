\documentclass[onecolumn, draftclsnofoot,10pt, compsoc]{IEEEtran}
\usepackage{url}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{epstopdf}
\epstopdfsetup{update} % only regenerate pdf files when eps file is newer
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{geometry}
\usepackage{color}
\usepackage{tikz}
\usepackage{rotating}
\usepackage{pgfgantt}
\usepackage{xcolor}
\geometry{textheight=9.5in, textwidth=7in}


% 1. Fill in these details
\def \CapstoneTeamName{		MAV Challenge}
\def \CapstoneTeamNumber{		32}
\def \GroupMemberOne{			Justin Sherburne}
\def \GroupMemberTwo{			Kaiyuan Fan}
\def \GroupMemberThree{			Yingshi Huang}
\def \CapstoneProjectName{		AHS Micro-Air Vehicle Challenge}
%\def \CapstoneSponsorCompany{	Columbia Helicopters}
\def \CapstoneSponsorPerson{		Nancy Squires, Ph.D.}

% 2. Uncomment the appropriate line below so that the document type works
\def \DocType{		%Problem Statement
				%Requirements Document
				%Technology Review
				%Design Document
				Midterm Report
				}
			
\newcommand{\NameSigPair}[1]{\par
\makebox[2.75in][r]{#1} \hfil 	\makebox[3.25in]{\makebox[2.25in]{\hrulefill} \hfill		\makebox[.75in]{\hrulefill}}
\par\vspace{-12pt} \textit{\tiny\noindent
\makebox[2.75in]{} \hfil		\makebox[3.25in]{\makebox[2.25in][r]{Signature} \hfill	\makebox[.75in][r]{Date}}}}
% 3. If the document is not to be signed, uncomment the RENEWcommand below
\renewcommand{\NameSigPair}[1]{#1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{titlepage}
    \pagenumbering{gobble}
    \begin{singlespace}
    	\includegraphics[height=4cm]{coe_v_spot1}
        \hfill 
        % 4. If you have a logo, use this includegraphics command to put it on the coversheet.
        %\includegraphics[height=4cm]{CompanyLogo}   
        \par\vspace{.2in}
        \centering
        \scshape{
            \huge CS Capstone \DocType \par
            {\large\today}\par
            \vspace{8pt}
            \textbf{\Huge\CapstoneProjectName}\par
			\vspace{1.5in}
            {\large Prepared for}\par
            % \Huge \CapstoneSponsorCompany\par
            % \vspace{5pt}
            {\Large\NameSigPair{\CapstoneSponsorPerson}\par}
			\vspace{3pt}
            {\large Prepared by }\par
            Group\CapstoneTeamNumber\par
            % 5. comment out the line below this one if you do not wish to name your team
            \CapstoneTeamName\par 
            \vspace{8pt}
            {\Large
                \NameSigPair{\GroupMemberOne}\par
                \NameSigPair{\GroupMemberTwo}\par
                \NameSigPair{\GroupMemberThree}\par
            }
            \vspace{.5in}
        }
        \begin{abstract}
        The purpose of this document is to provide a reflection on the progress of the Micro Air Vehicle project. We are now sixteen weeks into the project, approximately half of the way to completion. Here we will outline problems and possible solutions we have encountered this far, as well as a breakdown of weekly progress made thus far. 
        \end{abstract}     
    \end{singlespace}
\end{titlepage}
\newpage
\pagenumbering{arabic}
\tableofcontents
% 7. uncomment this (if applicable). Consider adding a page break.
%\listoffigures
%\listoftables
\clearpage


\section*{Revision History}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
		Name & Date & Reason For Changes & Version\\
        \hline
		Midterm Progress Report & Feb 15, 2018 & Initial Creation & 1.0\\
		\hline 
    \end{tabular}
\end{center}




\section{Purpose}

The purpose of this project is to design a vehicle capable of competing in the American Helicopter Society’s Micro-Air Vehicle challenge. We have elected to compete in the autonomous category, meaning our vehicle must be able to fly without any user interaction. We are representing Oregon State University for the first time at this competition, and if our project is successful we could gain additional outside funding similar to the OSU rocketry and solar teams.

\section{Project Goals}
The goal is to create a vehicle capable of navigating the competition environment without human control. Additionally, the vehicle should be able to pick up letters and deliver them to other locations within the competition environment. There are additional constraints on the size, weight, and specific functionalities of the vehicle, but from the computer science standpoint our goals are:

\begin{enumerate}
\item{The vehicle must be able to stream one camera feed to the base station for manual control. }
\item{We must have an emergency cut-off switch in case of a loss of communication or control. }
\item{We must have a manual override that will shut-down autonomous controls.}
\item{Ultrasonic sensors will be used to calculate distance from objects within the competition area in conjunction with the camera.}
\item{Image processing at minimum should be able to identify three objects: The letter, the landing area, and the boundary lines.}
\item{Any flight changes should originate from the base station, and motor controls should be implemented on the vehicle.}
\item{Our vehicle should fully comply with AHS competition rules and guidelines. }
\end{enumerate}

\section{Justin Sherburne's Report}

\subsection{Current Progress}

During the course of this term I have been developing our image recognition software for the competition. Currently I have implemented tracking based on object color. This should be sufficient for our autonomous design because the boundary is marked yellow, deliveries are red, and landing areas are black. It is possible to implement some form of pattern matching to fill in the gaps if the color is not enough by itself. Currently I am exploring an open-source guide written by Kyle Hounslow which fully implements object tracking based on color. Using this guide, we are able to identify individual colors in near real-time by filtering specific values from the video feed. Once these objects are tracked, their locations within the image frame are stored in a matrix. This matric is easily addressable and can be used by another function to predict where the aircraft should travel next. Because we had to remove one camera from our original design, the current plan for the autonomous flight will be to follow the boundary lines until we find one of our targets. 

Due to the complexity of the calculations being used for image processing, the majority of the processing is being done on the base station. Ultimately the code will also contain the controls feature and open a wireless link to the vehicle for control. It should require one parameter to be passed in at startup which is the IP of the vehicle (unless this IP can be fixed in the closed network). Streaming of the video occurs over the Wi-Fi Pineapple, which acts as a simple router for our purposes. This has been tested and works well in a variety of conditions. We will need to continue testing on the router in the future to make sure it can withstand long-distances and areas with strong network activity.


\subsection{Future Goals}

In the future I will continue to work on the image recognition side of the project and assist my teammates with their software when necessary. While I currently only have one color being filtered and targeted, I should be able to have several objects being tracked simultaneously by the end of the week. After that happens I will need to conduct some testing to see if we will also need to implement any pattern matching, or if we can move forward with just that. After the targets are identified, their location within the camera frame is stored in a matrix. This can be used very easily for logic controls and movement. 

The ultrasonic sensors will also need to be implemented on our design. We have not yet decided if that information will be relayed to the base station for calculations, or if it will remain on the PI zero controller. If it is local, we can use that information in real-time to avoid obstacles and track our height from the ground. Because the sensors have not yet been tested or installed on the aircraft we do not know what the distance measurements will be, or how that information will be best used.

\subsection{Problems}

One of the biggest problems I've encountered was in regard to the camera multiplexor we were planning on implementing. The documentation from the manufacturer wasn't completely correct, so it was not able to stream both camera’s at the same time. Instead you could only switch between the two feeds using the GPIO pins on the PI zero. Unfortunately for our situation if both cameras were not running simultaneously, then one of them was simply dead weight.

The second problem we've been experiencing has been from the ME and EE teams. Currently we are one month away from needing an alpha-level prototype for the competition. This means it is able to be flown, controlled manually, and have some autonomous implementation. The problem is that the ME and EE teams are still working on getting things implemented on the aircraft. The motors have not yet been wired to our motor controllers or attached to the PI Zero on-board. The ME team is still working on slight modifications to stability and balance, and the EE team needs to install voltage regulators and control modules. The CS team will have roughly two weeks to implement their progress on the aircraft before we need to demo for the competition.  

\subsection{Solutions}

Our solution to the camera problem had a couple of different options. The first option was to mount a second PI Zero to the vehicle. This is possible given our excess of thrust, but we would have also ran into issues with power and control. It is simple to have all of the components running to one PI Zero, but having two on the aircraft would unnecessarily complicate things. 

The second option, and the option we decided to take, was to lose a camera. This reduces the amount of information we are able to receive from the vehicle, but it also reduces the amount of network traffic and complexity of the project. In return we will need to rely more on our ultrasonic sensors and be a little more clever with our autonomous controls. The remaining camera will be directed straight down, and the aircraft will mostly trace the outside boundary, conducting a search pattern in the designated area.

The solution to the delay in hardware has been to do as much programming beforehand as possible. While we cannot yet build motor controls, we are able to press on with the image recognition and network communication portions. While some things are more challenging to implement without the physical device, many things can continue development without hardware. The CS team should have at least two weeks to implement as much functionality before we need to demo our aircraft for the competition.




\section{Yingshi Huang's Report}

\subsection{Current Progress}
The micro-helicopter has already selected all the hardware pieces of equipment which include two brush motors, a motor controller board, two ultrasonic sensors, a straightforward grasp motor, battery, a camera and the raspberry pi zero w. Our group has select every piece of the hardware considered by the weight of the micro-helicopter because of the rules of competition. All sections of the shell of the micro-helicopter are already stably combined which is 3D printed out and metal cut. And all the parts of the equipment have been mounted on the casing of the micro-helicopter. For the stabilization of driving of the micro-helicopter, our group design to add two equally square pieces at the tail of the micro-helicopter. Our group is still improving the better stabilization management of the whole micro-helicopter by moving the heavyweight parts of the micro-helicopter to the different place of micro-helicopter.
With a better stabilization, the micro-helicopter can drive comfortable, and the mounted sensors and the mounted camera can provide more accurate data for autonomous detection and calculation. The camera of the micro-helicopter can connect to the raspberry pi zero w. And the network of the raspberry pi zero w can provide the real-time video via the steam video software from the camera to raspberry pi zero w and then sent to laptop or desktop. Our group is planning to use the footage to detect the assigned places like target location, pick up location and destination. For the footage analysis, OpenCV is an open and wired source. In the library, template recognition and color selection are useful tools to analyze the streaming video. Currently, our group can use real-time laptop camera to find the boundary of the competition field which is separated by the two color and rhombus shape of the tape.

\subsection{Goals}
Our goal is to design and deliver a remotely piloted micro-helicopter or remotely autonomous micro-helicopter. The micro-helicopter our squad designed will be used to compete in micro air vehicle American helicopter society student challenge. In the application, our team designed to drive the micro-helicopter autonomously. First, the micro-helicopter will be able to detect the boundary of the competition field which is yellow and black tape. Second, the micro-helicopter will be ready to start from the assigned starting point and then drive and cover to the selected target point. After picking up the envelope on the ground from the selected target point, the micro-helicopter will put the envelope to the drop point. At the end of the task, the micro-helicopter will fly to the destination. In the meantime of flying movement, the micro-helicopter will perceive the obstacle in the field and avoid them.

\subsection{Problems \& Solutions}
To connect the hardware of the micro-helicopter, the using of raspberry pi zero w pins are necessary. Our group has found the connection of the ultrasonic sensors with raspberry pi zero w. Previously, the old series of ultrasonic sensors on the micro-helicopter cannot connect to raspberry pi zero w. However, with the released documentation of new series of the ultrasonic sensors have improved the accession of the hardware. The new selected ultrasonic sensor can detect objects from 1-mm to 500-cm, and the precise range is from 30-cm to 5-meters, If the distance detection of the ultrasonic sensor is closer than 30-cm, the data will appear as 30-cm.
In the project, our group still need to connect the remote controller and the raspberry pi zero w. The connection between ultrasonic sensors and raspberry pi zero w has to provide distance data for the calculations in the project. The motor controller also needs to be connected to the raspberry pi zero w. Our group is going to use the documentation of the hardware and then program the pins on raspberry pi zero w to send the data to the laptop to analyze.

\subsection{GPIO}
"General-purpose input/output (GPIO) is a generic pin on an integrated circuit or computer board." Our group has found the pin muxing of raspberry pi zero w on the documentation and reliable website. The aim of connecting pieces of the components is to customize the function of a GPIO pin on raspberry pi zero w for micro-helicopter. Every physical pin on the raspberry pi zero w has alternative functions. Some of the physical pins directly supplied high and no volts on the raspberry pi zero w. High volts is from 5v to 3.3v. On the raspberry pi zero w, there are two 5v pins and two 3.3v pins. The no volts which called ground pin occur eight pins on raspberry pi zero w.
While programmers code for the customizes project, they need to decide which kind of number module they are going to apply on the computer board. There are two ways of numbering the IO pins on using computer board. The first numbering system is BOARD, and the second is BCM. The advantage of the first system is no rewired connection with the code and hardware. For the benefit of the second system is more precise circuit diagram for the design. In the code of the project, the numbering system can set as "GPIO.setmode(GPIO.BOARD)" or "GPIO.setmode(GPIO.BCM)." The programmer can use "mode = GPIO.getmode()" to confirm the numbering system.
For the pins on the raspberry pi zero w, it is important to remember that pins can use as input as well as output. So secondary in the program is to set up the initial value. Assume the name of the initiated pin called channel so that "GPIO.setup(channel, GPIO.OUT, initial=GPIO.HIGH)". "GPIO.OUT" means data channel used as output value on raspberry pi zero w. "GPIO.HIGH" means data channel set to high volt on raspberry pi zero w. The other way to appear "GPIO.HIGH" is "1". At the end of the programming, simply use the command "GPIO.cleanup()" to clean up all the data and pins' memory on the raspberry pi zero w.The purpose of cleaning up the memory is to avoid any error in the next project to use the computer board.


\subsection{Code}
Here is the simple demo code \cite{r14} for set up and clean up.
\begin{flushleft}
import RPi.GPIO as GPIO
\end{flushleft}
\begin{flushleft}
\#import time library is for the sleep method
\end{flushleft}
\begin{flushleft}
import time
\end{flushleft}
\begin{flushleft}
button = 8
\end{flushleft}
\begin{flushleft}
\#set numbering mode for the program
\end{flushleft}
\begin{flushleft}
GPIO.setmode(GPIO.BOARD)
\end{flushleft}
\begin{flushleft}
\#setup button(pin8) as output pin
\end{flushleft}
\begin{flushleft}
GPIO.setup(button, GPIO.OUT, initial = 0)
\end{flushleft}
\begin{flushleft}
try:
\end{flushleft}
		\begin{flushleft}
    \quad \#turn on and off the same button in every 5 second
		\end{flushleft}
		\begin{flushleft}
    \quad while (True):
		\end{flushleft}
				\begin{flushleft}
        \quad \quad \#turn on, set as HIGH or 1
				\end{flushleft}
				\begin{flushleft}
				\quad \quad GPIO.output(button, GPIO.HIGH)
				\end{flushleft}
				\begin{flushleft}
				\quad \quad print("ON")
				\end{flushleft}
				\begin{flushleft}
				\quad \quad time.sleep(5)
				\end{flushleft}
				\begin{flushleft}
				\quad \quad \#turn off, set as LOW or 0
				\end{flushleft}
				\begin{flushleft}
				\quad \quad GPIO.output(button, GPIO.LOW)
				\end{flushleft}
				\begin{flushleft}
				\quad \quad print("OFF")
				\end{flushleft}
				\begin{flushleft}
				\quad \quad time.sleep(5)
				\end{flushleft}
\begin{flushleft}
except KeyboardInterrupt:
\end{flushleft}
	\begin{flushleft}
		\quad \#cleanup GPIO settings
		\end{flushleft}
		\begin{flushleft}
		\quad GPIO.cleanup()
		\end{flushleft}
		\begin{flushleft}
		\quad print("Exiting...")
	\end{flushleft}







\section{Kaiyuan Fan's Report}


\subsection{Current Progress}


In the past few weeks, I am working on the vehicle motor control part. The motor drives the flight of the helicopter. We will have two main coaxial blades and one rear blade on our helicopter which indicate there are three motors. Currently, I got one motor works by connecting it through motor control board L293D and a power module. All equipment and wirings are setting on a breadboard. Python has a package name “RPi.GPIO” to control Raspberry Pi GPIO channels. I implemented a python script by configuring four GPIO pins to control the DC motor. One GPIO pin is connected to the ground not actually act on the motor. One use for enabling and disabling the motor. Other two can act on changing the directions of the motor by setting the output signal level either high or low. Connecting Raspberry Pi to a Wi-Fi environment, I am able to change the rotation direction of the motor remotely. In Windows, for example, we can use Putty connect to the IP address of the Raspberry Pi then execute different codes.

The current implementation of driving one DC motor is based on the hardware in hand. L293D only support two motors and wiring are setting on a breadboard. The actual motor controller board we are going to use, and wiring set up will be decided by EE team. Changing the rotation direction of the DC motor is being tested, speed changing by setting PWM frequency will be tested in the future. On-helicopter motor testing will be started once motors and motor controller being wired by the EE team.


\subsection{Future Goals}


\begin{enumerate}
\item{Control three motors by the Raspberry Pi. Three motors need be able to run independently. }
\item{My current implementation is setting on the breadboard, the real aircraft wirings need to be set up nicely to balance the weight and keep the functionality.}
\item{Testing changes the PWM frequency to control the speed of the motor. Involving the speed relationship between two coaxial rotors to change the orientation of the helicopter.}
\item{Our helicopter is using the battery as the power supply. I am using a power module now; the battery needs set up with electronic speed control for a stable output. }
\item{Implementing manual flight mode, when helicopter receives the user input. Motors should be set to a certain mode. Specifically, when the aircraft was asked to perform takes off and hover, motors should rotate at the desired speed and keep the helicopter hover stably.}
\item{Implementing a reliable searching pattern algorithm of the helicopter to effectively cover the whole search area and find the target.}
\item{Implementing a reliable algorithm for the helicopter to avoid obstacles when the camera detects the obstacles.}
\item{In the autonomous package delivery mission. Implementing a feature to save the path of the helicopter, more specific, save the transitions of the motors to find the return routes between home base and the target search area.}
\end{enumerate}


\subsection{Problems}


The first problem is we don’t have our motors and motor controller wired on the helicopter. The hardware restricts testing the lifting, turning and further motor related actions of the helicopter.

The second problem is “RPi.GPIO” is unsuitable for real-time or timing-critical applications. This is because we cannot predict when Python will be busy garbage collecting. It also runs under the Linux kernel which is not suitable for real-time applications; it is multitasking O/S and another process may be given priority over the CPU, causing jitter in our program.

The third problem is there is a limited time remaining to implement the autonomous function and test the stability. We have around one month left to demonstrate the capabilities of vehicle autonomy and remote operation, including target tracking and package delivery.


\subsection{Solutions}


The solution of the first problem is to do as much as possible before the hardware available. We can collaborate with EE team to set up the electric circuits together. The coding portion of configuring Raspberry Pi’s GPIO pins is not hard. Setting PWM and pins output volts can be done once the circuit being constructed.

For the second problem, even though Python is easier for coding. The performance of the motors needs further testing. If the motor is unstable in a real-time response. We could change the coding in C/C++, C has a library “WiringPi” can be using to access GPIO pins of the Raspberry Pi.

To address the third problem, our big team are going to meet more often in the upcoming days. Individually, once I tested PWM frequency change control the speed of the motor in my hand. I will turn to work on making a GUI for user command communication and start to establish the target search pattern with my teammates.


\subsection{Other Relevant Information about Project}


There are many difficulties in the competition rules we can foresee: 
\begin{enumerate}
\item{Take off and steady-state hover based on the height level information that ultrasonic sensor given.}
\item{Find the landing base by the camera and successful landing.}
\item{Turning to the desired direction by setting different speeds of two coaxial rotors.}
\item{Recognize the rectangle obstacle and avoid it during the package delivery mission.}
\item{Recognize the first envelope, pick it up and drop to the target location. Pick the second envelope and return to the home base.}
\end{enumerate}

We have a video shows the flight capabilities of the aircraft due by 16 March 2018. Since we are choosing to establish an autonomous flight system, we must demonstrate the functionality of the autonomous flight. The whole project team will work more often together in the following month to hit the requirement of the competition.







\end{document}
